{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Deep Reinforcement Learning Nanodegree - Project 2: Continuous Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set everything up\n",
    "* Parameters\n",
    "* Unity environment\n",
    "* Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'buffer_size': 1e5,\n",
    "    'batch_size': 128,\n",
    "    'n_episodes': 300,\n",
    "    'max_t': 600,\n",
    "    'solution_threshold': 30.,\n",
    "    'eval_window_length': 100,\n",
    "    'num_agents': 20,\n",
    "    'agent_seed': 42,\n",
    "    'env_seed': 42,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 1e-3,\n",
    "    'lr_actor': 1e-4,\n",
    "    'lr_critic': 1e-4,\n",
    "    'critic_weight_decay': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app',\n",
    "                       seed=params['env_seed'],\n",
    "                       no_graphics=True)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.reset(train_mode=False)[brain_name].vector_observations[0].shape[0]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "agent = Agent(state_size=state_size,\n",
    "              action_size=action_size,\n",
    "              params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent with DDPG (Deep Deterministic Policy Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ddpg_training(env, brain_name, agent, params):\n",
    "    \"\"\"\n",
    "    Performs Deep Deterministic Policy Gradients Algorithm\n",
    "    according to\n",
    "    https://arxiv.org/pdf/1509.02971.pdf\n",
    "    \n",
    "    Args:\n",
    "        env (:obj:`UnityEnvironment`): Environment for agent(s) to act in\n",
    "            and to obtain observations (states) and rewards from\n",
    "        brain_name (str): name of the environment control brain\n",
    "        agent (:obj:`Agent.agent`): DDPG agent, i.e. actor and critic networks\n",
    "            as local and target versions)\n",
    "        params (dict): hyperparameters for instatiation, training and evaluation\n",
    "        \n",
    "    Returns:\n",
    "        avg_scores (list(float)): list of episode-wise scores averaged across agents\n",
    "    \"\"\"\n",
    "    eval_window_length = params['eval_window_length']\n",
    "    env_is_solved = False\n",
    "    avg_scores = []\n",
    "    scores_window = deque(maxlen=eval_window_length)\n",
    "    best_avg_agent_score = 0\n",
    "    \n",
    "    for i_episode in range(1, params['n_episodes']+1):\n",
    "        start = time.time()\n",
    "        agent.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        agent_scores = np.zeros(params['num_agents'])\n",
    "        \n",
    "        for t in range(params['max_t']):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            states = next_states\n",
    "            agent_scores += rewards\n",
    "        \n",
    "        avg_agent_score = np.mean(agent_scores)\n",
    "        avg_scores.append(avg_agent_score)\n",
    "        scores_window.append(avg_agent_score)\n",
    "        duration = int(time.time() - start)\n",
    "        \n",
    "        print('\\rEpisode {}\\t{} [s]\\tAverage Score: {:.2f}'.format(\n",
    "            i_episode, duration, avg_agent_score), end=\"\")\n",
    "        \n",
    "        if i_episode % params['eval_window_length'] == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        if (np.mean(scores_window) >= params['solution_threshold']) & (not env_is_solved):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode,\n",
    "                                                                                         np.mean(scores_window)))\n",
    "            env_is_solved = True\n",
    "        \n",
    "        # Save best agents\n",
    "        if env_is_solved & (avg_agent_score > best_avg_agent_score):\n",
    "            best_avg_agent_score = avg_agent_score\n",
    "            torch.save(agent.actor_local.state_dict(), 'best_actor_e_{}.pth'.format(i_episode))\n",
    "            torch.save(agent.critic_local.state_dict(), 'best_critic_e_{}.pth'.format(i_episode))\n",
    "            \n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores = perform_ddpg_training(env, brain_name, agent, params)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = np.array([np.mean(avg_scores[i:(i+params['eval_window_length'])])\n",
    "                           for i in range(len(avg_scores)-params['eval_window_length'])])\n",
    "steps = len(avg_scores)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.plot(np.arange(steps), avg_scores, linewidth=0.5)\n",
    "plt.plot(np.arange(params['eval_window_length'], steps), average_scores, 'g-')\n",
    "plt.plot(np.arange(steps), [params['solution_threshold']]*steps, 'r-')\n",
    "plt.ylabel('Score', fontsize=16)\n",
    "plt.xlabel('Episode #', fontsize=16)\n",
    "plt.legend(['Score', 'Average Score (w = 100)', 'Solution Threshold'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Environment was solved in Episode {}!\".format(\n",
    "    np.argmax((average_scores >= params['solution_threshold']))+params['eval_window_length']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch a trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app',\n",
    "                       seed=params['env_seed'],\n",
    "                       no_graphics=False)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_actor_filename = 'best_actor_e_2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.reset(train_mode=False)[brain_name].vector_observations[0].shape[0]\n",
    "action_size = brain.vector_action_space_size\n",
    "trained_agent = Agent(state_size=state_size,\n",
    "                      action_size=action_size,\n",
    "                      random_seed=params['agent_seed'])\n",
    "trained_agent.actor_local.load_state_dict(torch.load(best_actor_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "for t in range(500):\n",
    "    actions = trained_agent.act(states, add_noise=False)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    states = next_states\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
